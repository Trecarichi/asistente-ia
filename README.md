# üöÄ Asistente Inteligente de Clasificaci√≥n y Respuesta (LLM Robusto)

Este proyecto es un **Producto M√≠nimo Viable (MVP)** dise√±ado para automatizar la clasificaci√≥n de consultas no estructuradas (emails, tickets, chats) y generar respuestas informativas de primera l√≠nea utilizando un Modelo de Lenguaje Grande (LLM) desplegado localmente.

## ‚ú® Valor √önico: Robustez y Estabilidad

A diferencia de las soluciones demo, esta arquitectura est√° optimizada para la producci√≥n en entornos con recursos limitados.

El principal desaf√≠o t√©cnico resuelto fue el error de "Out of Memory" (`signal: killed`) com√∫n al correr modelos LLM en Docker.

- **Soluci√≥n de Ingenier√≠a:** Implementaci√≥n de **reserva de recursos fijos** (`deploy: resources:` en `docker-compose.yml`) para garantizar la estabilidad del servicio Ollama y prevenir fallos en la clasificaci√≥n, asegurando un **uptime continuo**.

## ‚öôÔ∏è Arquitectura T√©cnica

La soluci√≥n es 100% contenerizada, lo que garantiza la portabilidad y el despliegue On-Premise (en la infraestructura del cliente).

| Componente | Tecnolog√≠a | Prop√≥sito |
| :--- | :--- | :--- |
| **Frontend** | HTML/CSS/JavaScript | Interfaz de chat moderna y personalizable (Modo Oscuro incluido). |
| **Backend** | Python / Flask | API REST para manejar la sesi√≥n y la comunicaci√≥n con el LLM. |
| **LLM** | Ollama (Gemma 2B) | Servidor de inferencia del modelo de lenguaje. |
| **Orquestaci√≥n** | Docker Compose | Despliegue de los tres servicios con configuraci√≥n de recursos. |

## üí° Flujo de Trabajo (Prompt Engineering)

El backend utiliza una t√©cnica de **Prompt Engineering** que fuerza al LLM a devolver una estructura de datos JSON estandarizada, crucial para la integraci√≥n con sistemas de negocio (CRM, Ticketing):

```json
{
  "Clasificacion": "BENEFICIOS",
  "Urgencia": "2",
  "respuesta_extendida": "¬øQu√© beneficios te ofrece la escritura de tu propiedad?..."
}