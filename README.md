üöÄ Asistente Inteligente de Clasificaci√≥n y Respuesta (LLM Robusto con RAG)

Este proyecto es un Producto M√≠nimo Viable (MVP) dise√±ado para automatizar la clasificaci√≥n de consultas no estructuradas (emails, tickets, chats) y generar respuestas informativas de primera l√≠nea utilizando un Modelo de Lenguaje Grande (LLM) desplegado localmente.
‚ú® Valor √önico: Robustez y Estabilidad en Producci√≥n

A diferencia de las soluciones demo, esta arquitectura est√° optimizada para la producci√≥n en entornos con recursos limitados.
Desaf√≠o Resuelto	Soluci√≥n de Ingenier√≠a
Out of Memory (OOM) LLM Fallos (Problema com√∫n al correr modelos LLM en Docker).	Implementaci√≥n de reserva de recursos fijos (deploy: resources: en docker-compose.yml) para garantizar la estabilidad del servicio Ollama y asegurar un uptime continuo.
Respuestas Gen√©ricas/Sin Contexto (Problema de los LLM base).	Integraci√≥n de RAG (Retrieval-Augmented Generation): Uso de una base de datos SQLite para inyectar informaci√≥n espec√≠fica (ej: datos de municipios) al contexto del LLM, garantizando respuestas precisas y contextualizadas.
Salida No Est√°ndar (Fallos de JSON) (Problema de integraci√≥n con sistemas de negocio).	Mecanismo de Fallback JSON con Regex: Implementaci√≥n de un parser avanzado que usa expresiones regulares para extraer el objeto JSON, incluso si el LLM falla al envolverlo en texto o Markdown.
‚öôÔ∏è Arquitectura T√©cnica

La soluci√≥n es 100% contenerizada y utiliza tres microservicios, lo que garantiza la portabilidad y el despliegue On-Premise.
Componente	Tecnolog√≠a	Prop√≥sito Clave
Frontend	HTML/CSS/JS	Interfaz de chat personalizable con modo oscuro.
Backend	Python / Flask	API REST, l√≥gica de RAG y manejo de la sesi√≥n y LLM Fallback.
Datos RAG	SQLite / CSV	Fuente de datos para inyecci√≥n de contexto.
LLM	Ollama (Gemma 2B)	Servidor de inferencia del modelo de lenguaje.
Orquestaci√≥n	Docker Compose	Despliegue de los servicios con configuraci√≥n de recursos fijos.
üí° Flujo de Trabajo y Prompt Engineering

El backend utiliza t√©cnicas avanzadas para garantizar la fiabilidad del resultado:

    Generaci√≥n Aumentada (RAG): Si la consulta del usuario menciona una entidad clave (ej: un municipio o producto), el backend busca la informaci√≥n relevante en SQLite (datos_tierras.csv) y la a√±ade autom√°ticamente al prompt para el LLM.

    Salida Estructurada (JSON Mode): El prompt fuerza al LLM a devolver una estructura de datos JSON estandarizada, crucial para la integraci√≥n con sistemas de negocio (CRM, Ticketing):
    JSON

    {
      "Clasificacion": "BENEFICIOS",
      "Urgencia": "2",
      "respuesta_extendida": "¬øQu√© beneficios te ofrece la escritura de tu propiedad?..."
    }

¬øC√≥mo Empezar? (Instrucciones de Despliegue)

1. Requisitos:

    Docker y Docker Compose instalados.

    Conexi√≥n a internet estable (para descargar el modelo Gemma 2B la primera vez).

2. Despliegue de la Soluci√≥n (Un solo comando):
Bash

docker-compose up --build

    NOTA: El modelo Gemma 2B se descargar√° autom√°ticamente la primera vez.

3. Acceso a la Interfaz: Abra su navegador y acceda a: http://localhost:8080