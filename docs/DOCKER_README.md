# ğŸ³ GuÃ­a de Docker para Desarrollo Local

Esta guÃ­a explica cÃ³mo ejecutar el sistema IAMJus en un entorno Docker para desarrollo local.

## ğŸ“‹ Requisitos Previos

- Docker Desktop instalado (versiÃ³n 20.10 o superior)
- Docker Compose (incluido en Docker Desktop)
- **Servidor Ollama externo** corriendo con el modelo gemma2:9b
- Al menos 2GB de RAM disponible para Docker
- 2GB de espacio en disco para las imÃ¡genes

## ğŸ—ï¸ Arquitectura del Sistema

El sistema estÃ¡ compuesto por 2 servicios dockerizados + 1 servicio externo:

1. **Backend Flask** (puerto 8001): API REST que procesa las consultas (Docker)
2. **Frontend Nginx** (puerto 8080): Servidor web que sirve el HTML y hace proxy al backend (Docker)
3. **Ollama** (puerto 11434-11437): Servidor de IA externo (NO en Docker, corriendo nativamente)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Navegador â”‚
â”‚ localhost:8080 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nginx     â”‚â”€â”€â”€â”€â”€â–¶â”‚ Flask Backendâ”‚â”€â”€â”€â”€â”€â–¶â”‚ Ollama (Externo)â”‚
â”‚  (Frontend) â”‚      â”‚   (Python)   â”‚      â”‚  10.42.8.240    â”‚
â”‚   Docker    â”‚      â”‚    Docker    â”‚      â”‚   Nativo        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Inicio RÃ¡pido

### 1. Clonar y preparar el entorno

```bash
cd /ruta/al/proyecto/iamjus

# Copiar el archivo de ejemplo de variables de entorno
cp .env.example .env

# Editar .env y configurar las IPs de tu servidor Ollama
nano .env  # o usa tu editor preferido
```

**Importante:** AsegÃºrate de configurar correctamente las IPs en `.env`:
```bash
OLLAMA_ENDPOINT_1=http://TU_IP_OLLAMA:11434
OLLAMA_ENDPOINT_2=http://TU_IP_OLLAMA:11435
OLLAMA_ENDPOINT_3=http://TU_IP_OLLAMA:11436
OLLAMA_ENDPOINT_4=http://TU_IP_OLLAMA:11437
```

### 2. Construir y levantar los servicios

```bash
# Construir las imÃ¡genes y levantar todos los servicios
docker-compose up -d --build

# Ver los logs en tiempo real
docker-compose logs -f
```

### 3. Verificar conexiÃ³n con Ollama

```bash
# Probar que el backend puede conectarse a Ollama
make test-ollama

# O manualmente:
curl http://TU_IP_OLLAMA:11434/api/tags
```

### 4. Acceder a la aplicaciÃ³n

- **Frontend:** http://localhost:8080
- **Backend API:** http://localhost:8001

## ğŸ”§ Comandos Ãštiles

### Ver el estado de los servicios
```bash
docker-compose ps
```

### Ver logs de un servicio especÃ­fico
```bash
# Backend
docker-compose logs -f backend

# Frontend
docker-compose logs -f frontend
```

### Reiniciar un servicio
```bash
docker-compose restart backend
```

### Detener todos los servicios
```bash
docker-compose down
```

### Detener y eliminar volÃºmenes (limpieza completa)
```bash
docker-compose down -v
```

### Reconstruir un servicio especÃ­fico
```bash
docker-compose up -d --build backend
```

### Acceder a la shell de un contenedor
```bash
# Backend
docker exec -it iamjus-backend bash
```

## ğŸ› ï¸ Desarrollo

### Hot Reload

El cÃ³digo del backend estÃ¡ montado como volumen, por lo que los cambios se reflejan automÃ¡ticamente. Sin embargo, Flask no tiene hot reload habilitado por defecto en este setup.

Para habilitar hot reload en desarrollo, puedes modificar el `docker-compose.yml`:

```yaml
backend:
  command: flask run --host=0.0.0.0 --port=8001 --reload
```

### Modificar el HTML

Los archivos HTML estÃ¡n montados como volÃºmenes de solo lectura. Para ver cambios:

1. Modifica el archivo HTML en tu mÃ¡quina local
2. Recarga la pÃ¡gina en el navegador (Ctrl+F5 o Cmd+Shift+R)

### Base de datos

La base de datos SQLite (`municipios.db`) estÃ¡ montada como volumen, por lo que persiste entre reinicios.

Para regenerar la base de datos:
```bash
docker exec -it iamjus-backend rm /app/municipios.db
docker-compose restart backend
```

## ğŸ› Troubleshooting

### El backend no puede conectarse a Ollama

**Problema:** Error "No se pudo obtener respuesta de Ollama"

**SoluciÃ³n:**
```bash
# Verificar que Ollama estÃ© corriendo en el servidor externo
curl http://TU_IP_OLLAMA:11434/api/tags

# Verificar que las IPs en .env sean correctas
cat .env | grep OLLAMA_ENDPOINT

# Ver logs del backend para mÃ¡s detalles
docker-compose logs backend

# Verificar conectividad de red desde el contenedor
docker exec iamjus-backend curl http://TU_IP_OLLAMA:11434/api/tags
```

### El modelo no estÃ¡ disponible

**Problema:** Ollama responde pero no encuentra el modelo

**SoluciÃ³n:**
```bash
# Conectarse al servidor Ollama y verificar modelos instalados
ssh usuario@TU_IP_OLLAMA
ollama list

# Descargar el modelo si no estÃ¡ instalado
ollama pull gemma2:9b
```

### Puerto ya en uso

**Problema:** "Error: port is already allocated"

**SoluciÃ³n:**
```bash
# Identificar quÃ© proceso usa el puerto
lsof -i :8001  # o el puerto que estÃ© en conflicto

# Cambiar el puerto en docker-compose.yml
# Por ejemplo, cambiar "8001:8001" a "8002:8001"
```

### Problemas de permisos con la base de datos

**Problema:** "Permission denied" al escribir municipios.db

**SoluciÃ³n:**
```bash
# Dar permisos al archivo
chmod 666 municipios.db

# O ejecutar el contenedor con tu usuario
docker-compose down
# Agregar en docker-compose.yml bajo backend:
#   user: "${UID}:${GID}"
docker-compose up -d
```

### Contenedor se reinicia constantemente

**Problema:** El backend entra en loop de reinicio

**SoluciÃ³n:**
```bash
# Ver los logs para identificar el error
docker-compose logs backend

# Verificar que todas las dependencias estÃ©n instaladas
docker-compose exec backend pip list

# Reconstruir la imagen
docker-compose up -d --build backend
```

## ğŸ” ConfiguraciÃ³n de GPU en Servidor Ollama (Opcional)

Si tu servidor Ollama tiene GPU NVIDIA, puedes configurarla para mejor rendimiento:

1. En el servidor Ollama, instalar drivers NVIDIA y CUDA toolkit
2. Ollama detectarÃ¡ automÃ¡ticamente la GPU al correr nativamente
3. Verificar que Ollama estÃ© usando GPU:
```bash
# En el servidor Ollama
nvidia-smi  # Verificar que la GPU estÃ© disponible
ollama run gemma2:9b "test"  # DeberÃ­a usar GPU automÃ¡ticamente
```

## ğŸ“¦ ProducciÃ³n

Para producciÃ³n, se recomienda:

1. Usar un archivo `docker-compose.prod.yml` separado
2. Configurar variables de entorno para los endpoints reales de Ollama
3. Deshabilitar el debug de Flask
4. Usar volÃºmenes nombrados para persistencia
5. Configurar lÃ­mites de recursos
6. Implementar health checks mÃ¡s robustos

Ejemplo de `.env` para producciÃ³n:
```bash
OLLAMA_ENDPOINT_1=http://10.42.8.240:11434
OLLAMA_ENDPOINT_2=http://10.42.8.240:11435
OLLAMA_ENDPOINT_3=http://10.42.8.240:11436
OLLAMA_ENDPOINT_4=http://10.42.8.240:11437
FLASK_ENV=production
FLASK_DEBUG=0
```

## ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n de Docker Compose](https://docs.docker.com/compose/)
- [DocumentaciÃ³n de Ollama](https://github.com/ollama/ollama)
- [Flask en Docker](https://flask.palletsprojects.com/en/latest/deploying/docker/)

## ğŸ¤ Soporte

Si encuentras problemas no cubiertos en esta guÃ­a, por favor:

1. Revisa los logs: `docker-compose logs -f`
2. Verifica el estado: `docker-compose ps`
3. Consulta la documentaciÃ³n oficial de cada componente
